# lstm.yaml

model:
  class_path: lib.training.modules.base.PipelineModule
  init_args:
    model:
      class_path: lib.models.recurrent.RecurrentNetworkWithAttention
      init_args:
        embedding_dim: 64
        hidden_size: 64
        output_dim: 2
        num_layers: 8
        dropout: 0.4
        num_embeddings: 8
        num_heads: 8
        attention_dropout: 0.0
        num_concat_dims: 2
        pooling:
          type: mean
          dim: -2
          nonlinearity: exp
    objectives:
      loss:
        class_path: lib.training.optimisation.losses.LogMSELoss
      absolute_error:
        class_path: torch.nn.L1Loss
      relative_error:
        class_path: torchmetrics.MeanAbsolutePercentageError

data:
  batch_size: 64
  input_variables:
    - sequence_embeddings
    - structure_embeddings